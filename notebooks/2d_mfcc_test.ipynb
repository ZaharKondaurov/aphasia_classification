{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T17:58:17.359125Z",
     "start_time": "2025-04-02T17:58:12.449951Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import hub\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchaudio\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, balanced_accuracy_score, accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "import librosa\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "# from models.swishnet import SwishNet\n",
    "from models.cnn import CNNModel\n",
    "\n",
    "from src.utils import AphasiaDatasetMFCC, AphasiaDatasetSpectrogram, AphasiaDatasetWaveform\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# from torch_audiomentations import Compose, AddColoredNoise, PitchShift, Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1135bafdbf799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T17:58:17.524519Z",
     "start_time": "2025-04-02T17:58:17.360497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's cpu time!!!\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "AUDIO_LENGTH = 6_000\n",
    "SEQUENCE_LENGTH = 420\n",
    "MFCC = 128\n",
    "print(f\"It's {DEVICE} time!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62258711001a7dc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T17:58:17.545525Z",
     "start_time": "2025-04-02T17:58:17.525732Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc7c2b63b8acf07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:01:35.759968Z",
     "start_time": "2025-04-02T17:58:17.546806Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = AphasiaDatasetMFCC(os.path.join(DATA_DIR, \"train_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, mfcc=MFCC, n_mels=128, fft_size=512,\n",
    "                 hop_length=256, win_length=512, min_duration=10, max_duration=15, )\n",
    "test_dataset = AphasiaDatasetMFCC(os.path.join(DATA_DIR, \"val_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, mfcc=MFCC, n_mels=128, fft_size=512,\n",
    "                 hop_length=256, win_length=512, min_duration=10, max_duration=15)\n",
    "val_dataset = AphasiaDatasetMFCC(os.path.join(DATA_DIR, \"test_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, mfcc=MFCC, n_mels=128, fft_size=512,\n",
    "                 hop_length=256, win_length=512, min_duration=10, max_duration=15)\n",
    "\n",
    "# Балансировка классов для train\n",
    "train_labels = [label for _, label in train_dataset.data]\n",
    "class_counts = Counter(train_labels)\n",
    "if len(class_counts) < 2:\n",
    "    raise ValueError(\"Один из классов отсутствует в тренировочном наборе\")\n",
    "\n",
    "class_weights = {label: 1.0 / count for label, count in class_counts.items()}\n",
    "weights = [class_weights[label] for _, label in train_dataset.data]\n",
    "train_sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef4eb1d593d1278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:01:35.763684Z",
     "start_time": "2025-04-02T18:01:35.760886Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    if not batch:\n",
    "        return torch.zeros(0), torch.zeros(0)\n",
    "    \n",
    "    seq, labels = zip(*batch)\n",
    "    max_len = max(s.shape[1] for s in seq)\n",
    "\n",
    "    padded = torch.zeros(len(seq), MFCC, SEQUENCE_LENGTH)\n",
    "    for i, s in enumerate(seq):\n",
    "        padded[i, ..., :s.shape[-1]] = s[..., :SEQUENCE_LENGTH]\n",
    "    \n",
    "    return padded, torch.stack(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f7a59af4fd48cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:01:35.774366Z",
     "start_time": "2025-04-02T18:01:35.764400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, sampler=train_sampler, collate_fn=pad_sequence, drop_last=True, num_workers=6)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=pad_sequence, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=pad_sequence, drop_last=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27fcef8f1bd7ff26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:01:35.787727Z",
     "start_time": "2025-04-02T18:01:35.775362Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dl_train, dl_val, epochs=1, lr=0.001, device=\"cpu\"):\n",
    "      \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, threshold=1e-3)\n",
    "        \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training model\"):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        for features, target in dl_train:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features.unsqueeze(1)).squeeze()\n",
    "\n",
    "            preds = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
    "            train_acc.append(accuracy_score(target.cpu().detach().numpy(), preds))\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.detach().item()\n",
    "        \n",
    "        avg_train_acc = np.stack(train_acc, axis=0).mean()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, target in dl_val:\n",
    "                features, target = features.to(device), target.to(device)\n",
    "                \n",
    "                output = model(features.unsqueeze(1)).squeeze()\n",
    "                \n",
    "                preds = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
    "                val_acc.append(accuracy_score(target.cpu().detach().numpy(), preds))\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.detach().item()\n",
    "        \n",
    "        avg_val_acc = np.stack(val_acc, axis=0).mean()\n",
    "        avg_val_loss = total_val_loss / len(dl_val)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(avg_val_acc)\n",
    "        \n",
    "        if scheduler:\n",
    "            try:\n",
    "                scheduler.step()\n",
    "            except:\n",
    "                scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch}: train loss: {avg_train_loss:.3f}, train balanced acc: {avg_train_acc:.2f}, test loss: {avg_val_loss:.3f}, test balanced acc: {avg_val_acc:.2f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "            \n",
    "    return model, train_loss_list, val_loss_list, train_acc_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdda9e64e399b63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:01:35.805417Z",
     "start_time": "2025-04-02T18:01:35.788875Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, in_channels=1):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),  # Conv 1\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Conv 2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Conv 3\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Conv 4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Conv 5\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "        )\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Усреднение перед FC-слоями\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f9064ba68dad13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:01:35.827689Z",
     "start_time": "2025-04-02T18:01:35.807371Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn = CNNModel(num_classes=2, in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e723c8e3b5d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:06:16.503018Z",
     "start_time": "2025-04-02T18:01:35.829280Z"
    }
   },
   "outputs": [],
   "source": [
    "# cnn, train_l, val_l, train_accuracy, val_accuracy = train_model(cnn, train_dataloader, val_dataloader, epochs=10, lr=1e-3, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939c67b8aebdea58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:06:16.507564Z",
     "start_time": "2025-04-02T18:06:16.503754Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_data):\n",
    "    model = model.to(DEVICE)\n",
    "        \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for features, target in tqdm(test_data):\n",
    "            features = features.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            \n",
    "            label = model(features.unsqueeze(1)).to(\"cpu\").detach().numpy().squeeze()\n",
    "            preds.extend(label.argmax(axis=-1).tolist())\n",
    "            # print(target.shape)\n",
    "            targets.extend(target.to(\"cpu\").tolist())\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    # print(targets)\n",
    "    print(classification_report(targets, preds))    \n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2faf4577586ce552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:06:19.245896Z",
     "start_time": "2025-04-02T18:06:16.508551Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_model(cnn, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6299f4d3e02c5317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:06:19.633442Z",
     "start_time": "2025-04-02T18:06:19.246640Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "\n",
    "# axes[0][0].plot(train_l)\n",
    "# axes[0][0].set_title(\"Train loss\", fontsize=30)\n",
    "# axes[0][0].set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "# axes[0][1].plot(val_l)\n",
    "# axes[0][1].set_title(\"Validation loss\", fontsize=30)\n",
    "# axes[0][1].set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "# axes[1][0].plot(train_accuracy)\n",
    "# axes[1][0].set_title(\"Train accuracy\", fontsize=30)\n",
    "# axes[1][0].set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "# axes[1][1].plot(val_accuracy)\n",
    "# axes[1][1].set_title(\"Validation accuracy\", fontsize=30)\n",
    "# axes[1][1].set_xlabel(\"Epoch\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce235dd40c104ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:06:19.638471Z",
     "start_time": "2025-04-02T18:06:19.634497Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test_filenames.csv'))\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fe9e3d9e9197d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:06:19.659141Z",
     "start_time": "2025-04-02T18:06:19.639037Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model_for_each_participant(model, test_data):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(NORM_DIR, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(APHASIA_DIR, sgnl_path)\n",
    "                    \n",
    "\n",
    "                chunks = train_dataset.process_audio(sgnl_path)\n",
    "\n",
    "                padded = torch.zeros(len(chunks), MFCC, SEQUENCE_LENGTH)\n",
    "                for i, s in enumerate(chunks):\n",
    "\n",
    "                    padded[i, ..., :s.shape[-1]] = s[..., :SEQUENCE_LENGTH]\n",
    "                # print(model(torch.from_numpy(np.array(padded)).unsqueeze(1)).detach().numpy().shape)\n",
    "                pred = model(torch.from_numpy(np.array(padded)).unsqueeze(1)).detach().numpy()# .squeeze(1)# .argmax(axis=-1)\n",
    "                preds.append(pred)\n",
    "            labels = participant_samples[\"label\"]\n",
    "            # print(preds)\n",
    "            pred = np.concatenate(preds, axis=0).mean(axis=-2).argmax(axis=-1)\n",
    "\n",
    "            all_preds.append(pred)\n",
    "                \n",
    "            all_labels.append(labels.values[0])\n",
    "            errs = participant_samples[labels.values != pred]\n",
    "            if len(errs) > 0:\n",
    "                errors.append(errs)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    \n",
    "    return all_preds, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14afda000e47244d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:10:31.296956Z",
     "start_time": "2025-04-02T18:06:19.659995Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_model_for_each_participant(cnn, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e38f4c381b193e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:10:31.300311Z",
     "start_time": "2025-04-02T18:10:31.297582Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=2,):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.features = models.mobilenet_v3_large(pretrained=False)\n",
    "        \n",
    "        for child in self.features.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        in_features = self.features.classifier[0].in_features\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.features.classifier = self.classifier\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5cb49cc0a6f2c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:10:31.385295Z",
     "start_time": "2025-04-02T18:10:31.300868Z"
    }
   },
   "outputs": [],
   "source": [
    "mobilenet = MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6980c2ef1144e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:25.379297Z",
     "start_time": "2025-04-02T18:10:31.386043Z"
    }
   },
   "outputs": [],
   "source": [
    "# mobilenet, train_l, val_l, train_accuracy, val_accuracy = train_model(mobilenet, train_dataloader, val_dataloader, epochs=10, lr=1e-3, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc1c56f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sd = torch.load(os.path.join(os.getcwd(), \"checkpoints\", \"mobilenet_chkp\", \"mobilenet_0.94.pt\"), weights_only=False)\n",
    "\n",
    "mobilenet = MobileNet()\n",
    "mobilenet.load_state_dict(model_sd[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20d6c69255b7dc31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:25.792318Z",
     "start_time": "2025-04-02T18:11:25.380190Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:11<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.84      0.74       103\n",
      "           1       0.98      0.94      0.96       745\n",
      "\n",
      "    accuracy                           0.93       848\n",
      "   macro avg       0.82      0.89      0.85       848\n",
      "weighted avg       0.94      0.93      0.93       848\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = test_model(mobilenet, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fad54888d1987a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:26.158035Z",
     "start_time": "2025-04-02T18:11:25.793084Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "\n",
    "# axes[0][0].plot(train_l)\n",
    "# axes[0][0].set_title(\"Train loss\", fontsize=30)\n",
    "# axes[0][0].set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "# axes[0][1].plot(val_l)\n",
    "# axes[0][1].set_title(\"Validation loss\", fontsize=30)\n",
    "# axes[0][1].set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "# axes[1][0].plot(train_accuracy)\n",
    "# axes[1][0].set_title(\"Train accuracy\", fontsize=30)\n",
    "# axes[1][0].set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "# axes[1][1].plot(val_accuracy)\n",
    "# axes[1][1].set_title(\"Validation accuracy\", fontsize=30)\n",
    "# axes[1][1].set_xlabel(\"Epoch\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1882fb3d02f6a21c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:44.579487Z",
     "start_time": "2025-04-02T18:11:26.158907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]/tmp/ipykernel_109713/2403009599.py:37: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  pred = model(torch.from_numpy(np.array(padded)).unsqueeze(1)).detach().numpy()# .squeeze(1)# .argmax(axis=-1)\n",
      "100%|██████████| 72/72 [00:19<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87        21\n",
      "           1       0.93      0.98      0.95        51\n",
      "\n",
      "    accuracy                           0.93        72\n",
      "   macro avg       0.94      0.89      0.91        72\n",
      "weighted avg       0.93      0.93      0.93        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_arr, incorrect_samples = test_model_for_each_participant(mobilenet, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f449c07f9788172",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:44.585158Z",
     "start_time": "2025-04-02T18:11:44.580605Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def test_model_no_chunks(model, test_data):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds_aggr = []\n",
    "    all_labels_aggr = []\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds_aggr = []\n",
    "            labels_aggr = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(NORM_DIR, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(APHASIA_DIR, sgnl_path)\n",
    "                    \n",
    "                # y, sr = librosa.load(sgnl_path, sr=8_000)\n",
    "                audio = AudioSegment.from_file(sgnl_path, format=\"3gp\")\n",
    "                y = train_dataset.preprocess(audio)\n",
    "\n",
    "                pred = model(y[None, None, ...]).detach().numpy().squeeze().argmax(axis=-1)\n",
    "                \n",
    "                labels_aggr.append(participant_sample['label'])\n",
    "                preds_aggr.append(pred)\n",
    "\n",
    "            pred = scipy.stats.mode(np.array(preds_aggr))\n",
    "\n",
    "            all_preds_aggr.append(pred.mode)\n",
    "            all_labels_aggr.append(labels_aggr[0])\n",
    "            \n",
    "            all_preds.extend(preds_aggr)\n",
    "            all_labels.extend(labels_aggr)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(\"Without aggr\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    print(\"With aggr\")\n",
    "    print(classification_report(all_labels_aggr, all_preds_aggr))\n",
    "    \n",
    "    return all_preds, all_preds_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff77150c43150cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:57.657376Z",
     "start_time": "2025-04-02T18:11:44.585974Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:14<00:00,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82        42\n",
      "           1       0.93      0.97      0.95       130\n",
      "\n",
      "    accuracy                           0.92       172\n",
      "   macro avg       0.91      0.87      0.88       172\n",
      "weighted avg       0.92      0.92      0.92       172\n",
      "\n",
      "With aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85        21\n",
      "           1       0.92      0.96      0.94        51\n",
      "\n",
      "    accuracy                           0.92        72\n",
      "   macro avg       0.91      0.89      0.90        72\n",
      "weighted avg       0.92      0.92      0.92        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _ = test_model_no_chunks(mobilenet, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae8ae89905d18e",
   "metadata": {},
   "source": [
    "### False Negative and False Positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4ac7ac79b7774cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:57.661904Z",
     "start_time": "2025-04-02T18:11:57.657993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[               file_name  label     ID\n",
       " 4  N-0922-RAT-1-bike.wav      0  N0922\n",
       " 5  N-0922-RAT-1-robb.wav      0  N0922,\n",
       "                 file_name  label     ID\n",
       " 16  N-0923-RAT-1-bike.wav      0  N0923\n",
       " 17  N-0923-RAT-1-robb.wav      0  N0923,\n",
       "                 file_name  label     ID\n",
       " 18  N-1025-RAT-1-robb.wav      0  N1025\n",
       " 19  N-1025-RAT-1-bike.wav      0  N1025,\n",
       "                 file_name  label     ID\n",
       " 30  N-1015-RAT-1-robb.wav      0  N1015\n",
       " 31  N-1015-RAT-1-bike.wav      0  N1015,\n",
       "                 file_name  label    ID\n",
       " 138  A-215-RAT-1-robb.wav      1  A215\n",
       " 139  A-215-RAT-1-bike.wav      1  A215]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6eafa794cb11698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:57.682322Z",
     "start_time": "2025-04-02T18:11:57.662492Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_features = np.load(os.path.join(DATA_DIR, 'test_data_simple.npy'))\n",
    "test_filenames = pd.read_csv(os.path.join(DATA_DIR, 'test_filenames.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6871fb1ca2c3161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:57.697154Z",
     "start_time": "2025-04-02T18:11:57.683488Z"
    }
   },
   "outputs": [],
   "source": [
    "error_idxes = [y for x in incorrect_samples for y in x.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b79debfac9b7caac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:11:57.708384Z",
     "start_time": "2025-04-02T18:11:57.698919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.06080000e+05,  6.00000000e+00,  6.76800000e+04,\n",
       "        -4.05780000e+05,  7.00000000e+00, -5.79685714e+04,\n",
       "        -1.00073932e+00],\n",
       "       [ 2.30752000e+05,  5.00000000e+00,  4.61504000e+04,\n",
       "        -2.30452000e+05,  6.00000000e+00, -3.84086667e+04,\n",
       "        -1.00130179e+00],\n",
       "       [ 2.37440000e+05,  4.00000000e+00,  5.93600000e+04,\n",
       "        -2.37140000e+05,  5.00000000e+00, -4.74280000e+04,\n",
       "        -1.00126508e+00],\n",
       "       [ 9.32800000e+04,  5.00000000e+00,  1.86560000e+04,\n",
       "        -9.29800000e+04,  6.00000000e+00, -1.54966667e+04,\n",
       "        -1.00322650e+00],\n",
       "       [ 1.77024000e+05,  4.00000000e+00,  4.42560000e+04,\n",
       "        -1.76724000e+05,  5.00000000e+00, -3.53448000e+04,\n",
       "        -1.00169756e+00],\n",
       "       [ 1.96800000e+05,  2.00000000e+00,  9.84000000e+04,\n",
       "        -1.96500000e+05,  3.00000000e+00, -6.55000000e+04,\n",
       "        -1.00152672e+00],\n",
       "       [ 2.96928000e+05,  9.00000000e+00,  3.29920000e+04,\n",
       "        -2.96628000e+05,  1.00000000e+01, -2.96628000e+04,\n",
       "        -1.00101137e+00],\n",
       "       [ 1.69536000e+05,  6.00000000e+00,  2.82560000e+04,\n",
       "        -1.69236000e+05,  7.00000000e+00, -2.41765714e+04,\n",
       "        -1.00177267e+00],\n",
       "       [ 4.47616000e+05,  1.20000000e+01,  3.73013333e+04,\n",
       "        -4.47316000e+05,  1.30000000e+01, -3.44089231e+04,\n",
       "        -1.00067067e+00],\n",
       "       [ 2.62272000e+05,  4.00000000e+00,  6.55680000e+04,\n",
       "        -2.61972000e+05,  5.00000000e+00, -5.23944000e+04,\n",
       "        -1.00114516e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_features[np.array(error_idxes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34d106f41c62eaf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:15:00.254246Z",
     "start_time": "2025-04-02T18:15:00.233573Z"
    }
   },
   "outputs": [],
   "source": [
    "# CHECKPOINTS_DIR = os.path.join(\"..\", \"checkpoints\")\n",
    "# if not os.path.exists(os.path.join(CHECKPOINTS_DIR, \"mobilenet_chkp\")):\n",
    "#     os.makedirs(os.path.join(CHECKPOINTS_DIR, \"mobilenet_chkp\"))\n",
    "\n",
    "# torch.save(\n",
    "#     {\n",
    "#         'epoch': 10,\n",
    "#         'model_state_dict': mobilenet.state_dict(),\n",
    "#         # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         # 'scheduler_state_dict': scheduler.state_dict(),\n",
    "#         'losses_train': train_l,\n",
    "#         'losses_val': val_l,\n",
    "#     },\n",
    "#     os.path.join(CHECKPOINTS_DIR, \"mobilenet_chkp\", \"mobilenet\" + \"_\" + f\"{val_accuracy[-1]:.2f}\" + '.pt'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f9e3ef0b0eb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aphasia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
