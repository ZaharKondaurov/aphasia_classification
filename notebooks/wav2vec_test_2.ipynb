{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T16:30:09.143130Z",
     "start_time": "2025-03-27T16:30:02.366788Z"
    }
   },
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import hub\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, balanced_accuracy_score, accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "import librosa\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "from models.basic_transformer import BasicTransformer\n",
    "\n",
    "from src.utils import AphasiaDatasetMFCC, AphasiaDatasetSpectrogram, AphasiaDatasetWaveform\n",
    "\n",
    "from collections import Counter"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 19:30:06.896791: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-27 19:30:06.980427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743093007.029269   79148 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743093007.041129   79148 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-27 19:30:07.149152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:30:10.961792Z",
     "start_time": "2025-03-27T16:30:09.144242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "AUDIO_LENGTH = 6_000\n",
    "SEQUENCE_LENGTH = 31\n",
    "MFCC = 128\n",
    "print(f\"It's {DEVICE} time!!!\")"
   ],
   "id": "734d114c09bf7556",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's cuda time!!!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:30:10.973217Z",
     "start_time": "2025-03-27T16:30:10.963287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ],
   "id": "c588659242ba0e11",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.543978Z",
     "start_time": "2025-03-27T16:30:10.974816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = AphasiaDatasetWaveform(os.path.join(DATA_DIR, \"train_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000)\n",
    "test_dataset = AphasiaDatasetWaveform(os.path.join(DATA_DIR, \"val_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000)\n",
    "val_dataset = AphasiaDatasetWaveform(os.path.join(DATA_DIR, \"test_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000)\n",
    "\n",
    "# Балансировка классов для train\n",
    "train_labels = [label for _, label in train_dataset.data]\n",
    "class_counts = Counter(train_labels)\n",
    "if len(class_counts) < 2:\n",
    "    raise ValueError(\"Один из классов отсутствует в тренировочном наборе\")\n",
    "\n",
    "class_weights = {label: 1.0 / count for label, count in class_counts.items()}\n",
    "weights = [class_weights[label] for _, label in train_dataset.data]\n",
    "train_sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset), replacement=True)"
   ],
   "id": "5036894add804dac",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.546424Z",
     "start_time": "2025-03-27T16:33:05.544698Z"
    }
   },
   "cell_type": "code",
   "source": "MAX_LEN = 120_000",
   "id": "81bdff5397e1225b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.567527Z",
     "start_time": "2025-03-27T16:33:05.547012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_sequence(batch):\n",
    "    if not batch:\n",
    "        return torch.zeros(0), torch.zeros(0)\n",
    "    \n",
    "    seq, labels = zip(*batch)\n",
    "    # print(seq[1], labels)\n",
    "    max_len = max(s.shape[1] for s in seq)\n",
    "    # print(seq[0].shape)\n",
    "\n",
    "    # print(seq[0].shape)\n",
    "    padded = torch.zeros(len(seq), MAX_LEN)\n",
    "    for i, s in enumerate(seq):\n",
    "        padded[i, :s.shape[1]] = s[0, :MAX_LEN]\n",
    "    \n",
    "    return padded, torch.stack(labels) "
   ],
   "id": "8250a115b24637a6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.579282Z",
     "start_time": "2025-03-27T16:33:05.568949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, sampler=train_sampler, collate_fn=pad_sequence, drop_last=True, num_workers=6)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=pad_sequence, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=pad_sequence, drop_last=True, num_workers=6)"
   ],
   "id": "9510ca5946935d2f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.590484Z",
     "start_time": "2025-03-27T16:33:05.580777Z"
    }
   },
   "cell_type": "code",
   "source": "CHKP_PATH = os.path.join(\"..\", 'checkpoints', \"wav2vec_chkp\")",
   "id": "3665b2b6cbc3eef0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.286158Z",
     "start_time": "2025-03-27T16:33:05.592113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.wav2vecClassifier import Wav2vecClassifier\n",
    "\n",
    "wav2vec = Wav2vecClassifier(unfreeze=0.75)"
   ],
   "id": "961192ae1eb77e04",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.854463Z",
     "start_time": "2025-03-27T16:33:06.287385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_sd = torch.load(os.path.join(CHKP_PATH, \"wav2vec_0.75_0.9375.pt\"), weights_only=False)\n",
    "\n",
    "wav2vec.load_state_dict(model_sd[\"model_state_dict\"])"
   ],
   "id": "30736f256bf7877a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.857084Z",
     "start_time": "2025-03-27T16:33:06.855092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ],
   "id": "39422536076c6f3b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.868381Z",
     "start_time": "2025-03-27T16:33:06.857726Z"
    }
   },
   "cell_type": "code",
   "source": "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test_filenames.csv'))",
   "id": "34ddc81574cc6eee",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:51:55.476117Z",
     "start_time": "2025-03-27T15:51:55.467357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model_for_each_participant(model, test_data):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(NORM_DIR, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(APHASIA_DIR, sgnl_path)\n",
    "                    \n",
    "                chunks = train_dataset.process_audio(sgnl_path)\n",
    "\n",
    "                padded = torch.zeros(len(chunks), MAX_LEN)\n",
    "                for i, s in enumerate(chunks):\n",
    "                    padded[i, :s.shape[1]] = s[0, :MAX_LEN]\n",
    "                pred = model(torch.from_numpy(np.array(padded))).logits.detach().numpy().squeeze().argmax(axis=-1)\n",
    "                # print(type(pred))\n",
    "                if isinstance(pred, np.ndarray):\n",
    "                    # print(pred)\n",
    "                    preds.extend(pred.tolist())\n",
    "                else:\n",
    "                    preds.append(pred)\n",
    "            labels = participant_samples[\"label\"]\n",
    "  \n",
    "            # sgnls = torch.from_numpy(np.array(sgnls))\n",
    "            # preds = model(sgnls).detach().numpy().squeeze().argmax(axis=-1)\n",
    "            pred = scipy.stats.mode(np.array(preds))\n",
    "\n",
    "            all_preds.append(pred.mode)\n",
    "        \n",
    "            all_labels.append(labels.values[0])\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    return all_preds"
   ],
   "id": "ffb8aff7bca55e83",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:11:01.161118Z",
     "start_time": "2025-03-27T15:51:56.074121Z"
    }
   },
   "cell_type": "code",
   "source": "test_model_for_each_participant(wav2vec, test_data)",
   "id": "2c86b8e6cb64c628",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [19:05<00:00, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        21\n",
      "           1       0.94      1.00      0.97        51\n",
      "\n",
      "    accuracy                           0.96        72\n",
      "   macro avg       0.97      0.93      0.95        72\n",
      "weighted avg       0.96      0.96      0.96        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:38:28.796672Z",
     "start_time": "2025-03-27T16:38:28.788122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model_no_chunks(model, test_data):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds_aggr = []\n",
    "    all_labels_aggr = []\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds_aggr = []\n",
    "            labels_aggr = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(NORM_DIR, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(APHASIA_DIR, sgnl_path)\n",
    "                    \n",
    "                y, sr = librosa.load(sgnl_path, sr=8_000)\n",
    "\n",
    "                pred = model(torch.from_numpy(y)[None, :]).logits.detach().numpy().squeeze().argmax(axis=-1)\n",
    "                \n",
    "                # all_preds.append(pred)\n",
    "                labels_aggr.append(participant_sample['label'])\n",
    "                preds_aggr.append(pred)\n",
    "            # labels = participant_samples[\"label\"]\n",
    "  \n",
    "            # sgnls = torch.from_numpy(np.array(sgnls))\n",
    "            # preds = model(sgnls).detach().numpy().squeeze().argmax(axis=-1)\n",
    "            pred = scipy.stats.mode(np.array(preds_aggr))\n",
    "\n",
    "            # all_preds.append(pred.mode)\n",
    "        \n",
    "            # all_labels.append(labels.values[0])\n",
    "            all_preds_aggr.append(pred.mode)\n",
    "            all_labels_aggr.append(labels_aggr[0])\n",
    "            \n",
    "            all_preds.extend(preds_aggr)\n",
    "            all_labels.extend(labels_aggr)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(\"Without aggr\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    print(\"With aggr\")\n",
    "    print(classification_report(all_labels_aggr, all_preds_aggr))\n",
    "    \n",
    "    return all_preds, all_preds_aggr"
   ],
   "id": "c3ccbb9c7f0a39bd",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:59:38.248467Z",
     "start_time": "2025-03-27T16:38:28.987629Z"
    }
   },
   "cell_type": "code",
   "source": "_, _ = test_model_no_chunks(wav2vec, test_data)",
   "id": "bc00c21793c9f98d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [21:09<00:00, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        42\n",
      "           1       0.96      0.95      0.95       130\n",
      "\n",
      "    accuracy                           0.93       172\n",
      "   macro avg       0.90      0.91      0.91       172\n",
      "weighted avg       0.93      0.93      0.93       172\n",
      "\n",
      "With aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       0.96      0.94      0.95        51\n",
      "\n",
      "    accuracy                           0.93        72\n",
      "   macro avg       0.91      0.92      0.92        72\n",
      "weighted avg       0.93      0.93      0.93        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e447d7ad3edfb058"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
