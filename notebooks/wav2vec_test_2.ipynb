{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:30:09.143130Z",
     "start_time": "2025-03-27T16:30:02.366788Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import hub\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, balanced_accuracy_score, accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "import librosa\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "from models.basic_transformer import BasicTransformer\n",
    "\n",
    "from src.utils import AphasiaDatasetMFCC, AphasiaDatasetSpectrogram, AphasiaDatasetWaveform\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734d114c09bf7556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:30:10.961792Z",
     "start_time": "2025-03-27T16:30:09.144242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's cpu time!!!\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "AUDIO_LENGTH = 6_000\n",
    "SEQUENCE_LENGTH = 31\n",
    "MFCC = 128\n",
    "print(f\"It's {DEVICE} time!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c588659242ba0e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:30:10.973217Z",
     "start_time": "2025-03-27T16:30:10.963287Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5036894add804dac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.543978Z",
     "start_time": "2025-03-27T16:30:10.974816Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = AphasiaDatasetWaveform(os.path.join(DATA_DIR, \"train_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000)\n",
    "test_dataset = AphasiaDatasetWaveform(os.path.join(DATA_DIR, \"val_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000)\n",
    "val_dataset = AphasiaDatasetWaveform(os.path.join(DATA_DIR, \"test_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000)\n",
    "\n",
    "# Балансировка классов для train\n",
    "train_labels = [label for _, label in train_dataset.data]\n",
    "class_counts = Counter(train_labels)\n",
    "if len(class_counts) < 2:\n",
    "    raise ValueError(\"Один из классов отсутствует в тренировочном наборе\")\n",
    "\n",
    "class_weights = {label: 1.0 / count for label, count in class_counts.items()}\n",
    "weights = [class_weights[label] for _, label in train_dataset.data]\n",
    "train_sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bdff5397e1225b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.546424Z",
     "start_time": "2025-03-27T16:33:05.544698Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 120_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8250a115b24637a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.567527Z",
     "start_time": "2025-03-27T16:33:05.547012Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    if not batch:\n",
    "        return torch.zeros(0), torch.zeros(0)\n",
    "    \n",
    "    seq, labels = zip(*batch)\n",
    "    # print(seq[1], labels)\n",
    "    max_len = max(s.shape[1] for s in seq)\n",
    "    # print(seq[0].shape)\n",
    "\n",
    "    # print(seq[0].shape)\n",
    "    padded = torch.zeros(len(seq), MAX_LEN)\n",
    "    for i, s in enumerate(seq):\n",
    "        padded[i, :s.shape[1]] = s[0, :MAX_LEN]\n",
    "    \n",
    "    return padded, torch.stack(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9510ca5946935d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.579282Z",
     "start_time": "2025-03-27T16:33:05.568949Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, sampler=train_sampler, collate_fn=pad_sequence, drop_last=True, num_workers=6)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=pad_sequence, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=pad_sequence, drop_last=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3665b2b6cbc3eef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:05.590484Z",
     "start_time": "2025-03-27T16:33:05.580777Z"
    }
   },
   "outputs": [],
   "source": [
    "CHKP_PATH = os.path.join(os.getcwd(), 'checkpoints', \"wav2vec_chkp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961192ae1eb77e04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.286158Z",
     "start_time": "2025-03-27T16:33:05.592113Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from models.wav2vecClassifier import Wav2vecClassifier\n",
    "\n",
    "wav2vec = Wav2vecClassifier(unfreeze=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30736f256bf7877a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.854463Z",
     "start_time": "2025-03-27T16:33:06.287385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sd = torch.load(os.path.join(CHKP_PATH, \"wav2vec_0.75_0.9375.pt\"), weights_only=False)\n",
    "\n",
    "wav2vec.load_state_dict(model_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39422536076c6f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.857084Z",
     "start_time": "2025-03-27T16:33:06.855092Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ddc81574cc6eee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:33:06.868381Z",
     "start_time": "2025-03-27T16:33:06.857726Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test_filenames.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb8aff7bca55e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:51:55.476117Z",
     "start_time": "2025-03-27T15:51:55.467357Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model_for_each_participant(model, test_data):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(NORM_DIR, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(APHASIA_DIR, sgnl_path)\n",
    "                    \n",
    "                chunks = train_dataset.process_audio(sgnl_path)\n",
    "\n",
    "                padded = torch.zeros(len(chunks), MAX_LEN)\n",
    "                for i, s in enumerate(chunks):\n",
    "                    padded[i, :s.shape[1]] = s[0, :MAX_LEN]\n",
    "                pred = model(torch.from_numpy(np.array(padded))).logits.detach().numpy()# .squeeze()#.argmax(axis=-1)\n",
    "                # print(type(pred))\n",
    "                preds.append(pred)\n",
    "                # if isinstance(pred, np.ndarray):\n",
    "                #     # print(pred)\n",
    "                #     preds.extend(pred)\n",
    "                # else:\n",
    "                #     preds.append(pred)\n",
    "            labels = participant_samples[\"label\"]\n",
    "  \n",
    "            # sgnls = torch.from_numpy(np.array(sgnls))\n",
    "            # preds = model(sgnls).detach().numpy().squeeze().argmax(axis=-1)\n",
    "            pred = np.concatenate(preds, axis=0).mean(axis=-2).argmax(axis=-1)\n",
    "\n",
    "            all_preds.append(pred)\n",
    "        \n",
    "            all_labels.append(labels.values[0])\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c86b8e6cb64c628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:11:01.161118Z",
     "start_time": "2025-03-27T15:51:56.074121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]/tmp/ipykernel_130351/781085787.py:32: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  pred = model(torch.from_numpy(np.array(padded))).logits.detach().numpy()# .squeeze()#.argmax(axis=-1)\n",
      "100%|██████████| 72/72 [20:17<00:00, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.89        21\n",
      "           1       0.93      1.00      0.96        51\n",
      "\n",
      "    accuracy                           0.94        72\n",
      "   macro avg       0.96      0.90      0.93        72\n",
      "weighted avg       0.95      0.94      0.94        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_for_each_participant(wav2vec, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ccbb9c7f0a39bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:38:28.796672Z",
     "start_time": "2025-03-27T16:38:28.788122Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model_no_chunks(model, test_data):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds_aggr = []\n",
    "    all_labels_aggr = []\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds_aggr = []\n",
    "            labels_aggr = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(NORM_DIR, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(APHASIA_DIR, sgnl_path)\n",
    "                    \n",
    "                y, sr = librosa.load(sgnl_path, sr=8_000)\n",
    "\n",
    "                pred = model(torch.from_numpy(y)[None, :]).logits.detach().numpy().squeeze().argmax(axis=-1)\n",
    "                \n",
    "                # all_preds.append(pred)\n",
    "                labels_aggr.append(participant_sample['label'])\n",
    "                preds_aggr.append(pred)\n",
    "            # labels = participant_samples[\"label\"]\n",
    "  \n",
    "            # sgnls = torch.from_numpy(np.array(sgnls))\n",
    "            # preds = model(sgnls).detach().numpy().squeeze().argmax(axis=-1)\n",
    "            pred = scipy.stats.mode(np.array(preds_aggr))\n",
    "\n",
    "            # all_preds.append(pred.mode)\n",
    "        \n",
    "            # all_labels.append(labels.values[0])\n",
    "            all_preds_aggr.append(pred.mode)\n",
    "            all_labels_aggr.append(labels_aggr[0])\n",
    "            \n",
    "            all_preds.extend(preds_aggr)\n",
    "            all_labels.extend(labels_aggr)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(\"Without aggr\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    print(\"With aggr\")\n",
    "    print(classification_report(all_labels_aggr, all_preds_aggr))\n",
    "    \n",
    "    return all_preds, all_preds_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc00c21793c9f98d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:59:38.248467Z",
     "start_time": "2025-03-27T16:38:28.987629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [21:36<00:00, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        42\n",
      "           1       0.96      0.95      0.95       130\n",
      "\n",
      "    accuracy                           0.93       172\n",
      "   macro avg       0.90      0.91      0.91       172\n",
      "weighted avg       0.93      0.93      0.93       172\n",
      "\n",
      "With aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       0.96      0.94      0.95        51\n",
      "\n",
      "    accuracy                           0.93        72\n",
      "   macro avg       0.91      0.92      0.92        72\n",
      "weighted avg       0.93      0.93      0.93        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _ = test_model_no_chunks(wav2vec, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447d7ad3edfb058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aphasia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
