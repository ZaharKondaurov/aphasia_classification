{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T14:21:30.846261Z",
     "start_time": "2025-04-09T14:21:25.908401Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "from pyroomacoustics import room\n",
    "\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from random import uniform, choice\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import hub\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, balanced_accuracy_score, accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "import librosa\n",
    "import pyroomacoustics as pra \n",
    "\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "from models.basic_transformer import BasicTransformer\n",
    "\n",
    "from src.utils import AphasiaDatasetMFCC, AphasiaDatasetSpectrogram, AphasiaDatasetWaveform\n",
    "\n",
    "from collections import Counter"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 17:21:29.513452: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 17:21:29.524751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744208489.537358  114027 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744208489.541145  114027 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 17:21:29.553524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:21:30.886548Z",
     "start_time": "2025-04-09T14:21:30.847221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "AUDIO_LENGTH = 6_000\n",
    "SEQUENCE_LENGTH = 31\n",
    "MFCC = 128\n",
    "SR = 8_000\n",
    "print(f\"It's {DEVICE} time!!!\")"
   ],
   "id": "1afd44257f70c056",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's cuda time!!!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:21:30.897486Z",
     "start_time": "2025-04-09T14:21:30.887155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')\n",
    "NOISE_DIR = os.path.join(DATA_DIR, 'noise')\n",
    "RIR_DIR = os.path.join(DATA_DIR, 'RIRs')\n",
    "CHKP_PATH = os.path.join(\"..\", 'checkpoints', \"wav2vec_chkp\")"
   ],
   "id": "416c359b6b8f2294",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:21:30.919711Z",
     "start_time": "2025-04-09T14:21:30.898127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "signal_rir, sample_rate = torchaudio.load(\"/home/zakhar/ems_dereverb/data/data_thchs30/train/A2_0.wav\")\n",
    "if sample_rate != SR:\n",
    "    resampler = Resample(sample_rate, SR)\n",
    "    signal_rir = resampler(signal_rir)"
   ],
   "id": "fce2949ec015fcb4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:21:30.928759Z",
     "start_time": "2025-04-09T14:21:30.920962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WALLS_KEYWORDS = [\"hard_surface\", \"ceramic_tiles\", \"plasterboard\", \"wooden_lining\", \"glass_3mm\"]\n",
    "FLOOR_KEYWORDS = [\"linoleum_on_concrete\", \"carpet_cotton\"]\n",
    "CEILING_KEYWORDS = [\"ceiling_plasterboard\", \"ceiling_fissured_tile\", \"ceiling_metal_panel\", ]\n",
    "\n",
    "snr = (-5, 10)\n",
    "room_square = (7., 14.)\n",
    "room_height = (3., 4.)\n",
    "\n",
    "def simulate_rir_shoebox(signal: torch.Tensor) -> torch.Tensor:\n",
    "    square = uniform(*room_square)\n",
    "    width = uniform(2.5, square * 0.75)\n",
    "    length = square / width\n",
    "    height = uniform(*room_height)\n",
    "\n",
    "    rt60 = uniform(0.3, 1.25)   # Делать длинный ревёрб\n",
    "    room_dim = [length, width, height]\n",
    "\n",
    "    e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "\n",
    "    wall = pra.Material(choice(WALLS_KEYWORDS))\n",
    "    ceil = pra.Material(choice(CEILING_KEYWORDS))\n",
    "    floor = pra.Material(choice(FLOOR_KEYWORDS))\n",
    "\n",
    "    material = {\"east\": wall, \"west\": wall, \"north\": wall, \"south\": wall, \"ceiling\": ceil, \"floor\": floor}\n",
    "\n",
    "    room = pra.ShoeBox(room_dim, fs=SR, materials=material, max_order=max_order,\n",
    "                       use_rand_ism=True, max_rand_disp=0.05, ray_tracing=False)\n",
    "\n",
    "    source_locs = [uniform(0.01, length), uniform(0.01, width), uniform(1.0, 2.0)]\n",
    "    mic_locs = np.array([x * 0.98 for x in source_locs])[:, None]\n",
    "\n",
    "    room.add_source(source_locs, signal=signal.squeeze(), delay=0.5)\n",
    "\n",
    "    room.add_microphone_array(mic_locs)\n",
    "    room.compute_rir()\n",
    "    room.simulate()     # Внутри есть параметр snr, возможно, он пригодится\n",
    "\n",
    "    return room.rir[0][0]  "
   ],
   "id": "d34adb92d384b158",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:21:30.943588Z",
     "start_time": "2025-04-09T14:21:30.929544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(0, 100):\n",
    "    rir = simulate_rir_shoebox(signal_rir)\n",
    "    # print(type(rir[None, :]))\n",
    "    torchaudio.save(os.path.join(RIR_DIR, f'rir_{str(i)}'), src=torch.from_numpy(rir[None, :]), sample_rate=SR, format='wav')"
   ],
   "id": "ffec43d738467aa4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:21:30.955370Z",
     "start_time": "2025-04-09T14:21:30.944461Z"
    }
   },
   "cell_type": "code",
   "source": "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test_filenames.csv'))",
   "id": "e13c22a734e3dca2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:23:18.642964Z",
     "start_time": "2025-04-09T14:21:30.955965Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset = AphasiaDatasetWaveform(os.path.join(DATA_DIR, \"test_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, noise_dir=NOISE_DIR, add_noise=True, snr=(5, 20))",
   "id": "586ee454f140237b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:23:18.648235Z",
     "start_time": "2025-04-09T14:23:18.643779Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset[0][0]",
   "id": "64188b1dea88623c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100688])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:23:18.667552Z",
     "start_time": "2025-04-09T14:23:18.648991Z"
    }
   },
   "cell_type": "code",
   "source": "# from IPython.display import Audio",
   "id": "4a8d9872917df58e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:23:18.678948Z",
     "start_time": "2025-04-09T14:23:18.668584Z"
    }
   },
   "cell_type": "code",
   "source": "# Audio(test_dataset[0][0], rate=8_000)",
   "id": "cc137bc5f078e64e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wav2vec robustness test",
   "id": "2d8843fd57c267f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:23:19.903649Z",
     "start_time": "2025-04-09T14:23:18.680247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.wav2vecClassifier import Wav2vecClassifier\n",
    "\n",
    "wav2vec = Wav2vecClassifier(unfreeze=0.75)\n",
    "\n",
    "model_sd = torch.load(os.path.join(CHKP_PATH, \"wav2vec_0.75_0.9375.pt\"), weights_only=False)\n",
    "\n",
    "wav2vec.load_state_dict(model_sd[\"model_state_dict\"])"
   ],
   "id": "3c827b09f51ca50a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:33:44.534118Z",
     "start_time": "2025-04-09T15:33:44.529063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model(model, test_dataset, max_seq=420):\n",
    "    model = model.to(DEVICE)\n",
    "        \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for features, target in tqdm(test_dataset):\n",
    "            features = features.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            if model.__class__.__name__ == \"Wav2vecClassifier\":\n",
    "                label = model(features).logits.to(\"cpu\").detach().numpy().squeeze()\n",
    "            elif model.__class__.__name__ == \"MobileNet\":\n",
    "                label = model(features[None, ...]).to(\"cpu\").detach().numpy().squeeze()\n",
    "            else:\n",
    "                padded_features = torch.zeros(1, features.shape[0], max_seq, device=DEVICE)\n",
    "                padded_features[0, :, :features.shape[-1]] = features[..., :max_seq]\n",
    "                label = model(padded_features).to(\"cpu\").detach().numpy().squeeze()\n",
    "            preds.append(label.argmax(axis=-1))\n",
    "            targets.append(target.to(\"cpu\").item())\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    # print(targets)\n",
    "    print(classification_report(targets, preds))    \n",
    "\n",
    "    return preds"
   ],
   "id": "8bb5128c2096052",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:23:35.862741Z",
     "start_time": "2025-04-09T14:23:19.909071Z"
    }
   },
   "cell_type": "code",
   "source": "test_model(wav2vec, test_dataset)",
   "id": "b9b4d2449e4b9d5e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 955/955 [00:15<00:00, 60.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.75       136\n",
      "           1       0.96      0.96      0.96       819\n",
      "\n",
      "    accuracy                           0.93       955\n",
      "   macro avg       0.85      0.85      0.85       955\n",
      "weighted avg       0.93      0.93      0.93       955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:23:35.865876Z",
     "start_time": "2025-04-09T14:23:35.863336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 120_000\n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ],
   "id": "8df625bd48b72a50",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:34:35.037004Z",
     "start_time": "2025-04-09T15:34:35.029427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model_for_each_participant(model, test_data, test_dataset, norm_dir, aphasia_dir, max_seq=420):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(norm_dir, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(aphasia_dir, sgnl_path)\n",
    "                    \n",
    "                chunks = test_dataset.process_audio(sgnl_path)\n",
    "\n",
    "                if model.__class__.__name__ == \"Wav2vecClassifier\":\n",
    "                    padded = torch.zeros(len(chunks), MAX_LEN)\n",
    "                    for i, s in enumerate(chunks):\n",
    "                        padded[i, :s.shape[1]] = s[0, :MAX_LEN]\n",
    "                    pred = model(torch.from_numpy(np.array(padded))).logits.detach().numpy().squeeze().argmax(axis=-1)\n",
    "                elif model.__class__.__name__ == \"MobileNet\":\n",
    "                    padded = torch.zeros(len(chunks), MFCC, SEQUENCE_LENGTH)\n",
    "                    for i, s in enumerate(chunks):\n",
    "                        padded[i, ..., :s.shape[-1]] = s[..., :SEQUENCE_LENGTH]\n",
    "                    pred = model(torch.from_numpy(np.array(padded)).unsqueeze(1)).detach().numpy().squeeze().argmax(axis=-1)\n",
    "                else:\n",
    "                    padded = torch.zeros(len(chunks), MFCC, max_seq)\n",
    "                    for i, s in enumerate(chunks):\n",
    "                        padded[i, ..., :s.shape[-1]] = s[..., :max_seq]\n",
    "                    pred = model(torch.from_numpy(np.array(padded))).detach().numpy().squeeze().argmax(axis=-1)\n",
    "                # print(type(pred))\n",
    "                if isinstance(pred, np.ndarray):\n",
    "                    # print(pred)\n",
    "                    preds.extend(pred.tolist())\n",
    "                else:\n",
    "                    preds.append(pred)\n",
    "            labels = participant_samples[\"label\"]\n",
    "  \n",
    "            # sgnls = torch.from_numpy(np.array(sgnls))\n",
    "            # preds = model(sgnls).detach().numpy().squeeze().argmax(axis=-1)\n",
    "            pred = scipy.stats.mode(np.array(preds))\n",
    "\n",
    "            all_preds.append(pred.mode)\n",
    "        \n",
    "            all_labels.append(labels.values[0])\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    return all_preds"
   ],
   "id": "c0c98ca2447d4cc7",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:43:43.613046Z",
     "start_time": "2025-04-09T14:23:57.050228Z"
    }
   },
   "cell_type": "code",
   "source": "test_model_for_each_participant(wav2vec, test_data, test_dataset, NORM_DIR, APHASIA_DIR)",
   "id": "dd9626f2325b1b69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [19:46<00:00, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82        21\n",
      "           1       0.91      0.96      0.93        51\n",
      "\n",
      "    accuracy                           0.90        72\n",
      "   macro avg       0.90      0.86      0.88        72\n",
      "weighted avg       0.90      0.90      0.90        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:38:15.565648Z",
     "start_time": "2025-04-09T15:38:15.559557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def test_model_no_chunks(model, test_data, test_dataset, norm_dir, aphasia_dir, max_seq=420):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds_aggr = []\n",
    "    all_labels_aggr = []\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds_aggr = []\n",
    "            labels_aggr = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(norm_dir, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(aphasia_dir, sgnl_path)\n",
    "                    \n",
    "                # y, sr = librosa.load(sgnl_path, sr=8_000)\n",
    "                audio = AudioSegment.from_file(sgnl_path, format=\"3gp\")\n",
    "                y = test_dataset.preprocess(audio)\n",
    "                \n",
    "                if model.__class__.__name__ == \"Wav2vecClassifier\":\n",
    "                    pred = model(y).logits.detach().numpy().squeeze().argmax(axis=-1)\n",
    "                elif model.__class__.__name__ == \"MobileNet\":\n",
    "                    pred = model(y[None, None, ...]).detach().numpy().squeeze().argmax(axis=-1)\n",
    "                else:\n",
    "                    padded_y = torch.zeros(1, MFCC, max_seq)\n",
    "                    padded_y[..., :y.shape[-1]] = y[..., :max_seq]\n",
    "                    pred = model(padded_y).detach().numpy().squeeze().argmax(axis=-1)\n",
    "                \n",
    "                # all_preds.append(pred)\n",
    "                labels_aggr.append(participant_sample['label'])\n",
    "                preds_aggr.append(pred)\n",
    "            # labels = participant_samples[\"label\"]\n",
    "  \n",
    "            # sgnls = torch.from_numpy(np.array(sgnls))\n",
    "            # preds = model(sgnls).detach().numpy().squeeze().argmax(axis=-1)\n",
    "            pred = scipy.stats.mode(np.array(preds_aggr))\n",
    "\n",
    "            # all_preds.append(pred.mode)\n",
    "        \n",
    "            # all_labels.append(labels.values[0])\n",
    "            all_preds_aggr.append(pred.mode)\n",
    "            all_labels_aggr.append(labels_aggr[0])\n",
    "            \n",
    "            all_preds.extend(preds_aggr)\n",
    "            all_labels.extend(labels_aggr)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(\"Without aggr\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    print(\"With aggr\")\n",
    "    print(classification_report(all_labels_aggr, all_preds_aggr))\n",
    "    \n",
    "    return all_preds, all_preds_aggr"
   ],
   "id": "569d864f799062ca",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:06:18.699788Z",
     "start_time": "2025-04-09T14:44:43.732396Z"
    }
   },
   "cell_type": "code",
   "source": "test_model_no_chunks(wav2vec, test_data, test_dataset, NORM_DIR, APHASIA_DIR)",
   "id": "ee106875161a8a81",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [21:34<00:00, 17.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85        42\n",
      "           1       0.95      0.96      0.95       130\n",
      "\n",
      "    accuracy                           0.93       172\n",
      "   macro avg       0.91      0.90      0.90       172\n",
      "weighted avg       0.93      0.93      0.93       172\n",
      "\n",
      "With aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93        21\n",
      "           1       0.96      0.98      0.97        51\n",
      "\n",
      "    accuracy                           0.96        72\n",
      "   macro avg       0.96      0.94      0.95        72\n",
      "weighted avg       0.96      0.96      0.96        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:08:26.456048Z",
     "start_time": "2025-04-09T15:06:44.246826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')\n",
    "test_dataset_mfcc = AphasiaDatasetMFCC(os.path.join(DATA_DIR, \"test_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, noise_dir=NOISE_DIR, add_noise=True, snr=(5, 20))"
   ],
   "id": "8c5764cc471255f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MobileNet robustness test",
   "id": "d0ecd8df20191946"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:08:26.535068Z",
     "start_time": "2025-04-09T15:08:26.457117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.cnn import MobileNet\n",
    "\n",
    "mobilenet = MobileNet()\n",
    "\n",
    "CHKP_PATH = os.path.join(\"..\", 'checkpoints', \"mobilenet_chkp\")\n",
    "model_sd = torch.load(os.path.join(CHKP_PATH, \"mobilenet_0.94.pt\"), weights_only=False)\n",
    "\n",
    "mobilenet.load_state_dict(model_sd[\"model_state_dict\"])"
   ],
   "id": "1d1b6467d9c3558f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:08:29.114875Z",
     "start_time": "2025-04-09T15:08:26.535987Z"
    }
   },
   "cell_type": "code",
   "source": "test_model(mobilenet, test_dataset_mfcc)",
   "id": "1d18a189042c20fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 967/967 [00:02<00:00, 378.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.56      0.69       138\n",
      "           1       0.93      0.99      0.96       829\n",
      "\n",
      "    accuracy                           0.93       967\n",
      "   macro avg       0.91      0.77      0.82       967\n",
      "weighted avg       0.93      0.93      0.92       967\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:08:38.984489Z",
     "start_time": "2025-04-09T15:08:38.981723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ],
   "id": "3f21ea629e8b6071",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:15:34.479576Z",
     "start_time": "2025-04-09T15:12:57.471325Z"
    }
   },
   "cell_type": "code",
   "source": "test_model_for_each_participant(mobilenet, test_data, test_dataset_mfcc, NORM_DIR, APHASIA_DIR)",
   "id": "6b97d8442646188b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [02:36<00:00,  2.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.05      0.08        21\n",
      "           1       0.71      0.96      0.82        51\n",
      "\n",
      "    accuracy                           0.69        72\n",
      "   macro avg       0.52      0.50      0.45        72\n",
      "weighted avg       0.60      0.69      0.60        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:16:17.706006Z",
     "start_time": "2025-04-09T15:15:53.525398Z"
    }
   },
   "cell_type": "code",
   "source": "test_model_no_chunks(mobilenet, test_data, test_dataset_mfcc, NORM_DIR, APHASIA_DIR)",
   "id": "3c467a60b87ec1a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:24<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.40      0.57        42\n",
      "           1       0.84      0.99      0.91       130\n",
      "\n",
      "    accuracy                           0.85       172\n",
      "   macro avg       0.89      0.70      0.74       172\n",
      "weighted avg       0.86      0.85      0.82       172\n",
      "\n",
      "With aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.65        21\n",
      "           1       0.82      1.00      0.90        51\n",
      "\n",
      "    accuracy                           0.85        72\n",
      "   macro avg       0.91      0.74      0.77        72\n",
      "weighted avg       0.87      0.85      0.83        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:17:00.665503Z",
     "start_time": "2025-04-09T15:17:00.662405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ],
   "id": "3c6430f26f57f91d",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SwishNet robustness test",
   "id": "bb91e90528d60bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:29:22.353506Z",
     "start_time": "2025-04-09T15:29:22.345453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.swishnet import SwishNet\n",
    "SEQUENCE_LENGTH = 420\n",
    "MFCC = 128\n",
    "swishnet = SwishNet(MFCC, 2, input_size=SEQUENCE_LENGTH, dropout_rate=0.5)\n",
    "\n",
    "CHKP_PATH = os.path.join(\"..\", 'checkpoints', \"swishnet_chkp\")\n",
    "model_sd = torch.load(os.path.join(CHKP_PATH, \"swishnet_0.919_best.pt\"), weights_only=False)\n",
    "swishnet.load_state_dict(model_sd[\"model_state_dict\"])\n",
    "# swishnet = swishnet.cpu()"
   ],
   "id": "30d34c5b46f46a17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:33:49.178319Z",
     "start_time": "2025-04-09T15:33:48.253955Z"
    }
   },
   "cell_type": "code",
   "source": "test_model(swishnet, test_dataset_mfcc)",
   "id": "56e3f7fce2ea711e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 967/967 [00:00<00:00, 1063.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.30      0.38       138\n",
      "           1       0.89      0.96      0.92       829\n",
      "\n",
      "    accuracy                           0.86       967\n",
      "   macro avg       0.72      0.63      0.65       967\n",
      "weighted avg       0.84      0.86      0.85       967\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:35:29.138013Z",
     "start_time": "2025-04-09T15:35:29.135672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ],
   "id": "6af8b39b896208e9",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:36:51.563288Z",
     "start_time": "2025-04-09T15:35:29.879527Z"
    }
   },
   "cell_type": "code",
   "source": "test_model_for_each_participant(swishnet, test_data, test_dataset_mfcc, NORM_DIR, APHASIA_DIR)",
   "id": "1630ecff9358518b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [01:21<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.14      0.24        21\n",
      "           1       0.74      0.98      0.84        51\n",
      "\n",
      "    accuracy                           0.74        72\n",
      "   macro avg       0.74      0.56      0.54        72\n",
      "weighted avg       0.74      0.74      0.67        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:38:40.102764Z",
     "start_time": "2025-04-09T15:38:23.899152Z"
    }
   },
   "cell_type": "code",
   "source": "test_model_no_chunks(swishnet, test_data, test_dataset_mfcc, NORM_DIR, APHASIA_DIR)",
   "id": "438cf1b03aa3614a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:16<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.31      0.43        42\n",
      "           1       0.81      0.95      0.88       130\n",
      "\n",
      "    accuracy                           0.80       172\n",
      "   macro avg       0.75      0.63      0.65       172\n",
      "weighted avg       0.78      0.80      0.77       172\n",
      "\n",
      "With aggr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.43      0.56        21\n",
      "           1       0.80      0.96      0.88        51\n",
      "\n",
      "    accuracy                           0.81        72\n",
      "   macro avg       0.81      0.69      0.72        72\n",
      "weighted avg       0.81      0.81      0.78        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ab92a94a2a619fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
