{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-15T19:41:15.184099Z",
     "start_time": "2025-04-15T19:41:07.589074Z"
    }
   },
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import hub\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, balanced_accuracy_score, accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "import librosa\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "from models.basic_transformer import BasicTransformer\n",
    "\n",
    "from src.utils import AphasiaDatasetMFCC, AphasiaDatasetSpectrogram, AphasiaDatasetWaveform\n",
    "\n",
    "from collections import Counter\n",
    "from models.wav2vecClassifier import Wav2vecClassifier\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:41:12.859446: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-15 22:41:12.975621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744746073.040935   12758 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744746073.055814   12758 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-15 22:41:13.167011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T19:41:16.987191Z",
     "start_time": "2025-04-15T19:41:15.185275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "AUDIO_LENGTH = 6_000\n",
    "SEQUENCE_LENGTH = 31\n",
    "MFCC = 128\n",
    "print(f\"It's {DEVICE} time!!!\")"
   ],
   "id": "f5cbb9db80a265f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's cuda time!!!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T19:41:16.997799Z",
     "start_time": "2025-04-15T19:41:16.987966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')\n",
    "RIR_DIR = os.path.join(DATA_DIR, 'RIRs')\n",
    "NOISE_DIR = os.path.join(DATA_DIR, 'noise')"
   ],
   "id": "9bf6e28699c2b0fd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T19:41:17.009052Z",
     "start_time": "2025-04-15T19:41:16.998855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "augmentations = audio_augment = Compose([\n",
    "                AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.7),\n",
    "                TimeStretch(min_rate=0.4, max_rate=1.5, p=0.5),\n",
    "                PitchShift(min_semitones=-8, max_semitones=8, p=0.5),\n",
    "                Shift(min_shift=-0.8, max_shift=0.8, p=0.5),\n",
    "                Gain(min_gain_db=-14, max_gain_db=14, p=0.5),\n",
    "            ])"
   ],
   "id": "d3a50d55d592ee6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T19:41:18.667880Z",
     "start_time": "2025-04-15T19:41:17.009909Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset = AphasiaDatasetSpectrogram(os.path.join(DATA_DIR, \"train_filenames_mc_1.csv\"), VOICES_DIR, snr=(10, 20), rirs_dir=RIR_DIR, target_sample_rate=8_000, file_format=\"wav\", transforms=augmentations)",
   "id": "1df1668605f5a5f4",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mAphasiaDatasetSpectrogram\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain_filenames_mc_1.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mVOICES_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrirs_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRIR_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_sample_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8_000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwav\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maugmentations\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/aphasia_classification/src/utils.py:326\u001B[0m, in \u001B[0;36mAphasiaDatasetSpectrogram.__init__\u001B[0;34m(self, csv_file, root_dir, target_sample_rate, fft_size, hop_length, win_length, min_duration, max_duration, add_noise, snr, room_square, room_height, noise_dir, rirs_dir, file_format, transforms)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, csv_file, root_dir, target_sample_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16000\u001B[39m, fft_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m,\n\u001B[1;32m    317\u001B[0m              hop_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, win_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, min_duration\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, max_duration\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m,\n\u001B[1;32m    318\u001B[0m              add_noise: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    324\u001B[0m              file_format: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3gp\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    325\u001B[0m              transforms\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mAphasiaDatasetSpectrogram\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcsv_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroot_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_sample_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfft_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m             \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwin_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_duration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_duration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_noise\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroom_square\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mroom_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrirs_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_format\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransforms\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/aphasia_classification/src/utils.py:176\u001B[0m, in \u001B[0;36mAphasiaDataset.__init__\u001B[0;34m(self, csv_file, root_dir, target_sample_rate, fft_size, hop_length, win_length, min_duration, max_duration, add_noise, snr, room_square, room_height, noise_dir, rirs_dir, file_format, transforms)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file_path:\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m         segments \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mextend([(s, label) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m segments])\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/aphasia_classification/src/utils.py:295\u001B[0m, in \u001B[0;36mAphasiaDataset.process_audio\u001B[0;34m(self, file_path)\u001B[0m\n\u001B[1;32m    293\u001B[0m end \u001B[38;5;241m=\u001B[39m start \u001B[38;5;241m+\u001B[39m segment_duration\n\u001B[1;32m    294\u001B[0m segment \u001B[38;5;241m=\u001B[39m audio[start:end]\n\u001B[0;32m--> 295\u001B[0m preprocessed_segment \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43msegment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m preprocessed_segment \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    297\u001B[0m     segments\u001B[38;5;241m.\u001B[39mappend(preprocessed_segment)\n",
      "File \u001B[0;32m~/aphasia_classification/src/utils.py:350\u001B[0m, in \u001B[0;36mAphasiaDatasetSpectrogram.preprocess\u001B[0;34m(self, segment)\u001B[0m\n\u001B[1;32m    347\u001B[0m y \u001B[38;5;241m=\u001B[39m waveform\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 350\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_sample_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    352\u001B[0m spectrogram \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39mstft(y, n_fft\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfft_size, hop_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhop_length, win_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwin_length)\n\u001B[1;32m    353\u001B[0m mag \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mabs(spectrogram)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/aphasia_classification/.venv/lib/python3.12/site-packages/audiomentations/core/composition.py:130\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, samples, sample_rate)\u001B[0m\n\u001B[1;32m    128\u001B[0m         random\u001B[38;5;241m.\u001B[39mshuffle(transforms)\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m transforms:\n\u001B[0;32m--> 130\u001B[0m         samples \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m samples\n",
      "File \u001B[0;32m~/aphasia_classification/.venv/lib/python3.12/site-packages/audiomentations/core/transforms_interface.py:139\u001B[0m, in \u001B[0;36mBaseWaveformTransform.__call__\u001B[0;34m(self, samples, sample_rate)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupports_mono:\n\u001B[1;32m    134\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MonoAudioNotSupportedException(\n\u001B[1;32m    135\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m only supports multichannel audio, not mono audio\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[1;32m    137\u001B[0m             )\n\u001B[1;32m    138\u001B[0m         )\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m samples\n",
      "File \u001B[0;32m~/aphasia_classification/.venv/lib/python3.12/site-packages/audiomentations/augmentations/pitch_shift.py:66\u001B[0m, in \u001B[0;36mPitchShift.apply\u001B[0;34m(self, samples, sample_rate)\u001B[0m\n\u001B[1;32m     64\u001B[0m stretch\u001B[38;5;241m.\u001B[39mpreset(samples\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], sample_rate)\n\u001B[1;32m     65\u001B[0m stretch\u001B[38;5;241m.\u001B[39msetTransposeSemitones(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_semitones\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 66\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[43mstretch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m samples\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m original_ndim:\n\u001B[1;32m     68\u001B[0m     samples \u001B[38;5;241m=\u001B[39m samples[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_dataset = AphasiaDatasetSpectrogram(os.path.join(DATA_DIR, \"val_filenames_mc_1.csv\"), VOICES_DIR, target_sample_rate=8_000, file_format=\"wav\")",
   "id": "fdd179a92081f958",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "val_dataset = AphasiaDatasetSpectrogram(os.path.join(DATA_DIR, \"test_filenames_mc_1.csv\"), VOICES_DIR, target_sample_rate=8_000, file_format=\"wav\")",
   "id": "f66b1cf4329012fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Балансировка классов для train\n",
    "train_labels = [label for _, label in train_dataset.data]\n",
    "class_counts = Counter(train_labels)\n",
    "if len(class_counts) < 4:\n",
    "    raise ValueError(\"Один из классов отсутствует в тренировочном наборе\")\n",
    "\n",
    "class_weights = {label: 1.0 / count for label, count in class_counts.items()}\n",
    "weights = [class_weights[label] for _, label in train_dataset.data]\n",
    "train_sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset), replacement=True)"
   ],
   "id": "851fa58434ec86cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MAX_LEN = 120_000",
   "id": "9dc0434fa3945327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pad_sequence(batch):\n",
    "    if not batch:\n",
    "        return torch.zeros(0), torch.zeros(0)\n",
    "    \n",
    "    spectrograms, labels = zip(*batch)\n",
    "    max_len = max(s.shape[1] for s in spectrograms)\n",
    "    freq_bins = spectrograms[0].shape[2]\n",
    "    \n",
    "    padded = torch.zeros(len(spectrograms), 1, max_len, freq_bins)\n",
    "    for i, s in enumerate(spectrograms):\n",
    "        padded[i, :, :s.shape[1], :] = s\n",
    "    \n",
    "    return padded, torch.stack(labels) "
   ],
   "id": "f1f7b8d7e78bf5f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler, collate_fn=pad_sequence, drop_last=True, num_workers=6)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=pad_sequence, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=pad_sequence, drop_last=True, num_workers=6)"
   ],
   "id": "c7d6df4fa2bb4382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(model, dl_train, dl_val, epochs=1, lr=0.001, device=\"cpu\"):\n",
    "      \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()))).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "        \n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-7, last_epoch=-1) # torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, threshold=1e-3)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, threshold=1e-3)\n",
    "        \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training model\"):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        for features, target in dl_train:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features).squeeze()\n",
    "\n",
    "            preds = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
    "            train_acc.append(accuracy_score(target.cpu().detach().numpy(), preds))\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.detach().item()\n",
    "        \n",
    "        avg_train_acc = np.stack(train_acc, axis=0).mean()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, target in dl_val:\n",
    "                features, target = features.to(device), target.to(device)\n",
    "                \n",
    "                output = model(features).squeeze()\n",
    "                \n",
    "                preds = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
    "                val_acc.append(accuracy_score(target.cpu().detach().numpy(), preds))\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.detach().item()\n",
    "        \n",
    "        avg_val_acc = np.stack(val_acc, axis=0).mean()\n",
    "        avg_val_loss = total_val_loss / len(dl_val)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(avg_val_acc)\n",
    "        \n",
    "        if scheduler:\n",
    "            try:\n",
    "                scheduler.step()\n",
    "            except:\n",
    "                scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch}: train loss: {avg_train_loss:.3f}, train balanced acc: {avg_train_acc:.2f}, test loss: {avg_val_loss:.3f}, test balanced acc: {avg_val_acc:.2f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "            \n",
    "    return model, train_loss_list, val_loss_list, train_acc_list, val_acc_list"
   ],
   "id": "eca66fddd69a0d0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:19:38.600245Z",
     "start_time": "2025-04-15T15:19:38.530654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.cnn import MobileNet\n",
    "\n",
    "cnn = MobileNet(num_classes=4)"
   ],
   "id": "468792ab73ecb696",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:31:27.560786Z",
     "start_time": "2025-04-15T15:19:38.601296Z"
    }
   },
   "cell_type": "code",
   "source": "cnn, train_l, val_l, train_accuracy, val_accuracy = train_model(cnn, train_dataloader, val_dataloader, epochs=130, lr=1e-3, device=DEVICE)",
   "id": "3729fd0e068a8db6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   1%|          | 1/130 [00:13<28:26, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 1.360, train balanced acc: 0.26, test loss: 1.346, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   2%|▏         | 2/130 [00:26<27:55, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.314, train balanced acc: 0.28, test loss: 1.346, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   2%|▏         | 3/130 [00:39<27:36, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss: 1.268, train balanced acc: 0.33, test loss: 1.333, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   3%|▎         | 4/130 [00:52<27:19, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss: 1.229, train balanced acc: 0.37, test loss: 1.317, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   4%|▍         | 5/130 [01:05<27:03, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss: 1.211, train balanced acc: 0.39, test loss: 1.309, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   5%|▍         | 6/130 [01:18<26:52, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss: 1.156, train balanced acc: 0.40, test loss: 1.318, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   5%|▌         | 7/130 [01:31<26:41, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss: 1.122, train balanced acc: 0.41, test loss: 1.320, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   6%|▌         | 8/130 [01:44<26:27, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss: 1.085, train balanced acc: 0.43, test loss: 1.313, test balanced acc: 0.30, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   7%|▋         | 9/130 [01:57<26:14, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss: 1.083, train balanced acc: 0.43, test loss: 1.322, test balanced acc: 0.30, lr: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   8%|▊         | 10/130 [02:10<26:00, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss: 0.974, train balanced acc: 0.49, test loss: 1.398, test balanced acc: 0.19, lr: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   8%|▊         | 11/130 [02:23<25:47, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 0.897, train balanced acc: 0.56, test loss: 1.463, test balanced acc: 0.19, lr: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   9%|▉         | 12/130 [02:36<25:33, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss: 0.861, train balanced acc: 0.57, test loss: 1.598, test balanced acc: 0.19, lr: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  10%|█         | 13/130 [02:49<25:22, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss: 0.832, train balanced acc: 0.59, test loss: 1.640, test balanced acc: 0.19, lr: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  11%|█         | 14/130 [03:02<25:09, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train loss: 0.708, train balanced acc: 0.64, test loss: 1.592, test balanced acc: 0.25, lr: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  12%|█▏        | 15/130 [03:15<24:56, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss: 0.606, train balanced acc: 0.69, test loss: 1.738, test balanced acc: 0.36, lr: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  12%|█▏        | 16/130 [03:28<24:44, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train loss: 0.573, train balanced acc: 0.71, test loss: 1.884, test balanced acc: 0.30, lr: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  13%|█▎        | 17/130 [03:41<24:31, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train loss: 0.521, train balanced acc: 0.72, test loss: 1.888, test balanced acc: 0.31, lr: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  14%|█▍        | 18/130 [03:54<24:26, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train loss: 0.473, train balanced acc: 0.75, test loss: 2.020, test balanced acc: 0.31, lr: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  15%|█▍        | 19/130 [04:07<24:17, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train loss: 0.448, train balanced acc: 0.76, test loss: 2.056, test balanced acc: 0.36, lr: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  15%|█▌        | 20/130 [04:20<24:04, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train loss: 0.418, train balanced acc: 0.77, test loss: 2.284, test balanced acc: 0.33, lr: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  16%|█▌        | 21/130 [04:34<23:55, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train loss: 0.403, train balanced acc: 0.79, test loss: 2.180, test balanced acc: 0.37, lr: 6.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  17%|█▋        | 22/130 [04:47<23:43, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: train loss: 0.365, train balanced acc: 0.80, test loss: 2.358, test balanced acc: 0.36, lr: 6.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  18%|█▊        | 23/130 [05:00<23:33, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: train loss: 0.378, train balanced acc: 0.80, test loss: 2.460, test balanced acc: 0.36, lr: 6.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  18%|█▊        | 24/130 [05:13<23:21, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: train loss: 0.342, train balanced acc: 0.83, test loss: 2.386, test balanced acc: 0.35, lr: 6.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  19%|█▉        | 25/130 [05:27<23:15, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: train loss: 0.323, train balanced acc: 0.83, test loss: 2.542, test balanced acc: 0.37, lr: 3.125e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  20%|██        | 26/130 [05:40<22:55, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: train loss: 0.324, train balanced acc: 0.84, test loss: 2.614, test balanced acc: 0.38, lr: 3.125e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  21%|██        | 27/130 [05:53<22:41, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: train loss: 0.327, train balanced acc: 0.84, test loss: 2.644, test balanced acc: 0.36, lr: 3.125e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  22%|██▏       | 28/130 [06:07<22:38, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: train loss: 0.330, train balanced acc: 0.84, test loss: 2.578, test balanced acc: 0.35, lr: 3.125e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  22%|██▏       | 29/130 [06:20<22:22, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: train loss: 0.317, train balanced acc: 0.84, test loss: 2.575, test balanced acc: 0.36, lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  23%|██▎       | 30/130 [06:33<22:11, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: train loss: 0.319, train balanced acc: 0.84, test loss: 2.674, test balanced acc: 0.38, lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  24%|██▍       | 31/130 [06:47<21:55, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: train loss: 0.301, train balanced acc: 0.85, test loss: 2.664, test balanced acc: 0.38, lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  25%|██▍       | 32/130 [07:00<21:42, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: train loss: 0.338, train balanced acc: 0.83, test loss: 2.674, test balanced acc: 0.38, lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  25%|██▌       | 33/130 [07:13<21:29, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: train loss: 0.312, train balanced acc: 0.84, test loss: 2.714, test balanced acc: 0.39, lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  26%|██▌       | 34/130 [07:26<21:16, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: train loss: 0.314, train balanced acc: 0.83, test loss: 2.723, test balanced acc: 0.39, lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  27%|██▋       | 35/130 [07:40<21:07, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: train loss: 0.292, train balanced acc: 0.85, test loss: 2.728, test balanced acc: 0.38, lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  28%|██▊       | 36/130 [07:53<20:54, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: train loss: 0.302, train balanced acc: 0.86, test loss: 2.730, test balanced acc: 0.38, lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  28%|██▊       | 37/130 [08:07<20:45, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: train loss: 0.327, train balanced acc: 0.85, test loss: 2.784, test balanced acc: 0.38, lr: 3.90625e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  29%|██▉       | 38/130 [08:20<20:27, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: train loss: 0.274, train balanced acc: 0.87, test loss: 2.780, test balanced acc: 0.37, lr: 3.90625e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  30%|███       | 39/130 [08:33<20:12, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: train loss: 0.271, train balanced acc: 0.87, test loss: 2.767, test balanced acc: 0.37, lr: 3.90625e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  31%|███       | 40/130 [08:47<20:03, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: train loss: 0.278, train balanced acc: 0.88, test loss: 2.768, test balanced acc: 0.37, lr: 3.90625e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  32%|███▏      | 41/130 [09:00<19:41, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: train loss: 0.308, train balanced acc: 0.85, test loss: 2.760, test balanced acc: 0.38, lr: 1.953125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  32%|███▏      | 42/130 [09:13<19:29, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: train loss: 0.285, train balanced acc: 0.86, test loss: 2.786, test balanced acc: 0.37, lr: 1.953125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  33%|███▎      | 43/130 [09:26<19:13, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: train loss: 0.279, train balanced acc: 0.87, test loss: 2.777, test balanced acc: 0.37, lr: 1.953125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  34%|███▍      | 44/130 [09:40<19:02, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: train loss: 0.264, train balanced acc: 0.88, test loss: 2.776, test balanced acc: 0.38, lr: 1.953125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  35%|███▍      | 45/130 [09:53<18:53, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: train loss: 0.263, train balanced acc: 0.88, test loss: 2.779, test balanced acc: 0.38, lr: 9.765625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  35%|███▌      | 46/130 [10:06<18:36, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: train loss: 0.292, train balanced acc: 0.87, test loss: 2.778, test balanced acc: 0.38, lr: 9.765625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  36%|███▌      | 47/130 [10:20<18:22, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: train loss: 0.301, train balanced acc: 0.85, test loss: 2.771, test balanced acc: 0.38, lr: 9.765625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  37%|███▋      | 48/130 [10:33<18:14, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: train loss: 0.291, train balanced acc: 0.86, test loss: 2.760, test balanced acc: 0.38, lr: 9.765625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  38%|███▊      | 49/130 [10:46<18:00, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: train loss: 0.306, train balanced acc: 0.85, test loss: 2.776, test balanced acc: 0.38, lr: 4.8828125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  38%|███▊      | 50/130 [11:00<17:44, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: train loss: 0.270, train balanced acc: 0.88, test loss: 2.781, test balanced acc: 0.37, lr: 4.8828125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  39%|███▉      | 51/130 [11:13<17:32, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: train loss: 0.287, train balanced acc: 0.86, test loss: 2.791, test balanced acc: 0.38, lr: 4.8828125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  40%|████      | 52/130 [11:27<17:25, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: train loss: 0.293, train balanced acc: 0.86, test loss: 2.793, test balanced acc: 0.37, lr: 4.8828125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  41%|████      | 53/130 [11:40<17:09, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: train loss: 0.307, train balanced acc: 0.85, test loss: 2.802, test balanced acc: 0.38, lr: 2.44140625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  41%|████      | 53/130 [11:48<17:09, 13.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cnn, train_l, val_l, train_accuracy, val_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m130\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 29\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, dl_train, dl_val, epochs, lr, device)\u001B[0m\n\u001B[1;32m     25\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     27\u001B[0m output \u001B[38;5;241m=\u001B[39m model(features)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m---> 29\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     30\u001B[0m train_acc\u001B[38;5;241m.\u001B[39mappend(accuracy_score(target\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(), preds))\n\u001B[1;32m     32\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, target)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "68ec7695383f424c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
